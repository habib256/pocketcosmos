# Guide d'Entra√Ænement de l'Agent IA pour la Simulation de Fus√©e

## üöÄ Vue d'ensemble

Ce guide explique comment utiliser le syst√®me d'entra√Ænement IA int√©gr√© pour entra√Æner un agent capable de contr√¥ler une fus√©e dans la simulation spatiale. L'agent utilise un algorithme DQN (Deep Q-Network) avec TensorFlow.js.

## üìã Pr√©requis

### ‚úÖ Composants Impl√©ment√©s

- **RocketAI.js** : Agent IA avec r√©seau de neurones DQN
- **HeadlessRocketEnvironment.js** : Environnement de simulation sans rendu graphique
- **TrainingOrchestrator.js** : Orchestrateur d'entra√Ænement avec m√©triques et contr√¥les
- **training-interface.html** : Interface web pour surveiller l'entra√Ænement
- **train.js** : Scripts de d√©monstration et d'entra√Ænement

### ‚ö° Nouvelles Fonctionnalit√©s

1. **Entra√Ænement Headless** : Simulation rapide sans rendu visuel
2. **Interface de Monitoring** : Suivi en temps r√©el des m√©triques
3. **Sauvegarde Automatique** : Checkpoints et mod√®les optimaux
4. **Early Stopping** : Arr√™t automatique si pas d'am√©lioration
5. **√âvaluation P√©riodique** : Tests sur environnement s√©par√©
6. **Configuration Flexible** : Hyperparam√®tres ajustables

## üéØ Comment D√©marrer l'Entra√Ænement

### M√©thode 1 : Interface Web

1. Ouvrir `training-interface.html` dans un navigateur
2. Ajuster les param√®tres d'entra√Ænement :
   - **√âpisodes** : 500-1000 pour un bon entra√Ænement
   - **Taux d'apprentissage** : 0.001-0.005
   - **Epsilon** : Commencer √† 1.0 pour l'exploration
   - **Objectif** : Choisir entre orbite, atterrissage, exploration
3. Cliquer sur "üéØ D√©marrer l'Entra√Ænement"
4. Surveiller les m√©triques en temps r√©el

### M√©thode 2 : Console du Navigateur

```javascript
// Entra√Ænement complet (500 √©pisodes)
demonstrateTraining();

// Entra√Ænement rapide (100 √©pisodes)
quickTraining();

// Test de performance de l'environnement
benchmarkEnvironment();
```

### M√©thode 3 : Code Personnalis√©

```javascript
// Configuration personnalis√©e
const customConfig = {
    maxEpisodes: 1000,
    learningRate: 0.002,
    epsilon: 1.0,
    targetSuccessRate: 0.8,
    objective: 'orbit'
};

// D√©marrage
const eventBus = new EventBus();
const trainer = new TrainingOrchestrator(eventBus);
await trainer.startTraining(customConfig);
```

## üìä M√©triques d'Entra√Ænement

### M√©triques Principales

- **R√©compense Moyenne** : Progression de l'apprentissage
- **Taux de Succ√®s** : Pourcentage d'√©pisodes r√©ussis
- **Exploration (Œµ)** : Taux d'exploration vs exploitation
- **Perte d'Entra√Ænement** : Qualit√© de l'apprentissage du r√©seau

### Crit√®res de Succ√®s

| Objectif | Crit√®re de Succ√®s | R√©compense Typique |
|----------|-------------------|-------------------|
| **Orbite** | Maintenir une orbite stable | > 50 points |
| **Atterrissage** | Atterrir avec vitesse < 5 m/s | > 100 points |
| **Exploration** | Maximiser la distance parcourue | Variable |

## ‚öôÔ∏è Configuration des Hyperparam√®tres

### Param√®tres Recommand√©s

```javascript
const OPTIMAL_CONFIG = {
    // Dur√©e d'entra√Ænement
    maxEpisodes: 1000,
    maxStepsPerEpisode: 2000,
    
    // R√©seau de neurones
    learningRate: 0.001,      // Plus faible = apprentissage stable
    batchSize: 64,            // Plus grand = apprentissage stable
    
    // Exploration vs Exploitation
    epsilon: 1.0,             // Exploration initiale maximale
    epsilonMin: 0.1,          // Exploration minimale finale
    epsilonDecay: 0.995,      // D√©croissance progressive
    
    // Apprentissage par renforcement
    gamma: 0.99,              // Facteur de d√©compte futur
    replayBufferSize: 50000,  // M√©moire des exp√©riences
    
    // √âvaluation et sauvegarde
    evaluationInterval: 100,   // √âvaluer tous les 100 √©pisodes
    checkpointInterval: 500,   // Sauvegarder tous les 500 √©pisodes
    targetSuccessRate: 0.8     // Objectif de 80% de succ√®s
};
```

### Ajustement selon les Performances

| Probl√®me | Solution |
|----------|----------|
| **Apprentissage trop lent** | ‚Üë learningRate, ‚Üì epsilon decay |
| **Instabilit√©** | ‚Üì learningRate, ‚Üë batchSize |
| **Pas d'exploration** | ‚Üë epsilon, ‚Üì epsilon decay |
| **Surapprentissage** | ‚Üì replay buffer, + early stopping |

## üî¨ Analyse des R√©sultats

### Graphiques de Performance

L'interface affiche deux graphiques principaux :

1. **R√©compense au fil du temps** : Doit √™tre croissante
2. **Perte d'entra√Ænement** : Doit √™tre d√©croissante et se stabiliser

### Diagnostic des Probl√®mes

#### Courbe de R√©compense Plate
```
Causes possibles :
- Fonction de r√©compense mal calibr√©e
- Taux d'apprentissage trop faible
- Probl√®me de normalisation des √©tats
```

#### Instabilit√© dans l'Apprentissage
```
Solutions :
- R√©duire le taux d'apprentissage
- Augmenter la taille du batch
- Ajuster le gamma (facteur de d√©compte)
```

#### Agent Bloqu√© dans un Comportement
```
Solutions :
- Augmenter l'exploration (epsilon)
- Modifier la fonction de r√©compense
- Red√©marrer avec des poids al√©atoires
```

## üéÆ Utilisation de l'Agent Entra√Æn√©

### Activation du Contr√¥le IA

1. **Dans le jeu principal** :
   - Appuyer sur la touche `I` pour activer/d√©sactiver l'IA
   - L'IA prend le contr√¥le de la fus√©e automatiquement

2. **Via le code** :
   ```javascript
   // Activer l'IA
   eventBus.emit(EVENTS.AI.TOGGLE_CONTROL);
   
   // Charger un mod√®le pr√©-entra√Æn√©
   await rocketAI.loadModel();
   ```

### Test de Performance

```javascript
// Tester l'agent sur plusieurs missions
for (let mission of ['orbit', 'land', 'explore']) {
    rocketAI.setObjective(mission);
    // Lancer la simulation et observer les r√©sultats
}
```

## üîß D√©pannage

### Erreurs Communes

#### "Module not found" ou Scripts non charg√©s
```
Solution : V√©rifier l'ordre de chargement dans index.html
L'ordre correct est :
1. EventTypes.js
2. Mod√®les (models/)
3. Contr√¥leurs de base (controllers/)
4. HeadlessRocketEnvironment.js
5. TrainingOrchestrator.js
6. train.js
```

#### Entra√Ænement Tr√®s Lent
```
Solutions :
- Utiliser le mode headless (sans rendu)
- R√©duire maxStepsPerEpisode
- Augmenter batchSize pour moins d'updates
- Fermer l'onglet de visualisation pendant l'entra√Ænement
```

#### Mod√®le ne se Sauvegarde Pas
```
V√©rifications :
- LocalStorage disponible dans le navigateur
- Pas de mode navigation priv√©e
- Suffisamment d'espace de stockage
```

## üìà Performance Attendue

### Benchmarks Typiques

| M√©trique | Valeur Attendue | Temps Estim√© |
|----------|----------------|--------------|
| **100 √©pisodes** | 20-30% succ√®s | 2-5 minutes |
| **500 √©pisodes** | 60-80% succ√®s | 10-20 minutes |
| **1000 √©pisodes** | 80-95% succ√®s | 20-40 minutes |

### Facteurs Influen√ßant la Performance

- **CPU/GPU** : WebGL acc√©l√®re TensorFlow.js
- **Complexit√© de l'objectif** : Orbite < Atterrissage < Exploration complexe
- **Configuration** : Hyperparam√®tres optimaux essentiels

## üöÄ Prochaines √âtapes

### Am√©liorations Possibles

1. **Architectures avanc√©es** :
   - R√©seaux convolutionnels pour l'analyse visuelle
   - LSTM pour la m√©moire temporelle
   - Actor-Critic au lieu de DQN

2. **Environnements plus complexes** :
   - Obstacles spatiaux
   - Multiples corps c√©lestes
   - Missions avec contraintes temporelles

3. **Apprentissage multi-objectifs** :
   - Optimiser plusieurs crit√®res simultan√©ment
   - Transfert d'apprentissage entre missions

4. **Interface avanc√©e** :
   - Visualisation de l'espace des √©tats
   - Analyse des strat√©gies apprises
   - Comparaison de diff√©rents mod√®les

## üìö Ressources Suppl√©mentaires

### Documentation Technique

- **TensorFlow.js** : https://www.tensorflow.org/js
- **Reinforcement Learning** : Sutton & Barto "Reinforcement Learning: An Introduction"
- **DQN Paper** : "Playing Atari with Deep Reinforcement Learning" (Mnih et al.)

### Exemples et Tutoriels

- **TensorFlow.js Examples** : https://github.com/tensorflow/tfjs-examples
- **DQN Implementation** : Voir `controllers/RocketAI.js`
- **Training Loop** : Voir `controllers/TrainingOrchestrator.js`

---

## üéØ R√©sum√© pour D√©marrer Rapidement

1. **Ouvrir** `training-interface.html`
2. **Configurer** 500 √©pisodes, objectif "orbit"
3. **D√©marrer** l'entra√Ænement
4. **Surveiller** la progression (r√©compense croissante)
5. **Tester** l'agent dans le jeu principal (touche `I`)

**Temps estim√© pour un agent fonctionnel : 15-30 minutes** 